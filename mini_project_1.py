# -*- coding: utf-8 -*-
"""mini_project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oySCb_eHTS5wDTFl1WxKGUJJPczZoPFf
"""

import numpy as np
import random
import torchvision
from torchvision import transforms as transforms
from torch import nn
from torch.nn import functional as F
from math import cos
from math import pi
from torchsummary import summary

import os
import time
import torch
import math
import matplotlib.pyplot as plt

# set seeds 
torch.cuda.empty_cache()
torch.manual_seed(2221)
torch.cuda.manual_seed_all(2221)

# Apply normalization and HorizontalFlip to the data for augmentation transform

aug_train = transforms.Compose([
    transforms.RandomCrop(32,padding=4,padding_mode='reflect'),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) 
    ])

aug_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) 
    ])

# load data

train_data = torchvision.datasets.CIFAR10('./CIFAR10',train=True,download=True,transform=aug_train)
test_data = torchvision.datasets.CIFAR10('./CIFAR10',train=False,download=True,transform=aug_test)
batch_size = 128
train_dataloader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=False)
train_size = len(train_dataloader)
test_size = len(test_dataloader)

class BasicBlock(nn.Module):

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out



class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(self.in_planes)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
       # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)

        self.linear = nn.Linear(1024, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out
    




def decay_learning_rate(optimizer, current, max_epoch, min_rate, max_rate):
    # quicklly decrease the learning rate in the first 10 epoches
    if current < 10:
        lr = max_rate * current / 10
    # applying COSINEANNEALINGWARMRESTARTS in later epoches
    else:
        lr = min_rate + (max_rate-min_rate)*(1 + cos(pi * (current - 10) / (max_epoch - 10))) / 2
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

# dispalying the model details   Total params: 3,154,762

model = ResNet(BasicBlock, [3,3,2]).cuda()
summary(model, input_size=(3,32,32))

epoch_num = 20
record = 'record_model.pt'

train_loss_history = []
test_loss_history = []
test_accuracy_history = []
train_accuracy_history = []

# used in dynamically learning rate adjustment
Loss = torch.nn.CrossEntropyLoss()
max_rate = 0.1 
min_rate=0.001

# train using different network
optimizer = torch.optim.SGD(model.parameters(),lr=max_rate,momentum=0.9,nesterov=True)
# optimizer = torch.optim.Adam(model.parameters(),lr=max_rate, betas=(0.9,0.999), eps=1e-08, amsgrad=True)

# load from the stored model
if os.path.exists(record):
  loaded_model = torch.load(record)
  model.load_state_dict(loaded_model['model_state_dict'])
  optimizer.load_state_dict(loaded_model['optimizer_state_dict'])
  Loss = loaded_model['Loss']
  train_loss_history = loaded_model['train_loss_history']
  test_loss_history = loaded_model['test_loss_history']
  test_accuracy_history = loaded_model['test_accuracy_history']
  train_accuracy_history = loaded_model['train_accuracy_history']

# otherwise, train the model from scratch     
else:
  for epoch in range(1, epoch_num + 1):
    train_loss = 0
    test_loss = 0
    train_accuracy = 0
    test_accuracy = 0
    for i, data in enumerate(train_dataloader):
      inputs = data[0].cuda()
      labels = data[1].cuda()
      total_num = len(labels)
      optimizer.zero_grad()
      predicted = model(inputs)
      max_prob = torch.max(predicted,1)[1]
      loss = Loss(predicted,labels)
      loss.backward()
      decay_learning_rate(optimizer,epoch,epoch_num,min_rate,max_rate)
      optimizer.step()
      train_loss += loss.item()
      epoc_acc = torch.eq(max_prob,labels).sum()/ total_num
      train_accuracy += (100* epoc_acc).data.cpu().numpy()

    for i, data in enumerate(test_dataloader):
      with torch.no_grad():
        inputs = data[0].cuda()
        labels = data[1].cuda()
        total_num = len(labels)
        optimizer.zero_grad()
        predicted = model(inputs)
        max_prob = torch.max(predicted,1)[1]
        loss = Loss(predicted,labels)
        test_loss += loss.item()
        epoc_acc = torch.eq(max_prob,labels).sum()/ total_num
        test_accuracy += (100* epoc_acc).data.cpu().numpy()

    train_loss = train_loss/train_size
    test_loss = test_loss/test_size
    train_accuracy = train_accuracy/train_size
    test_accuracy = test_accuracy/test_size

    train_loss_history.append(train_loss)
    test_loss_history.append(test_loss)
    train_accuracy_history.append(train_accuracy)

    test_accuracy_history.append(test_accuracy)

    print('Epoch#{:3}, Train accuracy = {:.2f}%, Test accuracy =  {:.2f}%'.format(epoch,train_accuracy,test_accuracy))

    # store the current status 
    torch.save({'epoch':epoch,
            'model_state_dict':model.state_dict(),
            'optimizer_state_dict':optimizer.state_dict(),
            'Loss':Loss,
            'train_loss_history':train_loss_history,
            'test_loss_history':test_loss_history,
            'test_accuracy_history':test_accuracy_history,
            'train_accuracy_history':train_accuracy_history},record)

# plot the loss and accuracy curve of differnt models

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plt.plot(range(1,epoch_num+1),train_loss_history,label='Train loss')
plt.plot(range(1,epoch_num+1),test_loss_history,label='Test loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.xlim([0, epoch_num])
plt.legend()


plt.subplot(1,2,2)
plt.plot(range(1,epoch_num+1),train_accuracy_history,label='Train accuracy')
plt.plot(range(1,epoch_num+1),test_accuracy_history,label='Test accuracy')
plt.xlim([0, epoch_num])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

print('Accuracy =', "{:.2f}%".format(max(test_accuracy_history)))